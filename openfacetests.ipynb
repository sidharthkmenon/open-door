{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ketanagrawal/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import openface\n",
    "from keras.models import load_model\n",
    "from keras.utils import CustomObjectScope\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import dlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ketanagrawal/anaconda2/lib/python2.7/site-packages/keras/engine/saving.py:270: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "with CustomObjectScope({'tf': tf}):\n",
    "    model = load_model('./nn4.small2.v1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/obieda01/Deep-Learning-Specialization-Coursera/blob/master/Course%204%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face%20Recognition%20for%20the%20Happy%20House%20-%20%20v1.ipynb\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines)\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.squared_difference(anchor, positive))\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.squared_difference(anchor, negative))\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    maxi = tf.maximum(basic_loss, 0.0)\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam= keras.optimizers.Adam()\n",
    "model.compile(optimizer='adam', loss=triplet_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding(img_path, model):\n",
    "    start = time.time()\n",
    "    img1 = cv2.imread(img_path, 1)\n",
    "    x, y, w, h = detect_largest_face(img1)\n",
    "    print \"Face detection took %s secs\" % (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "#     cv2.imshow('largest face', img1[y:y+h, x:x+w])\n",
    "#     cv2.waitKey()\n",
    "#     if img_to_encoding.align is None:\n",
    "#         facePredictor = '/Users/ketanagrawal/openface/models/dlib/shape_predictor_68_face_landmarks.dat'\n",
    "#         img_to_encoding.align = openface.AlignDlib(facePredictor)\n",
    "#     print \"Face alignment part 1 took %s secs\" % (time.time() - start)\n",
    "#     s = time.time()\n",
    "    bb = dlib.rectangle(x, y, x + w, y + h)\n",
    "    img1 = img_to_encoding.align.align(96, img1, bb, landmarkIndices=openface.AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "#     print \"Face alignment part 2 took %s secs\" % (time.time() - s)\n",
    "    print \"Face alignment took %s secs\" % (time.time() - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    img = img1[...,::-1]\n",
    "    img = np.around(img/255.0, decimals=12)\n",
    "    x_train = np.array([img])\n",
    "    embedding = model.predict_on_batch(x_train)\n",
    "    print \"Forward pass took %s secs\" % (time.time() - start)\n",
    "    return embedding\n",
    "\n",
    "facePredictor = '/Users/ketanagrawal/openface/models/dlib/shape_predictor_68_face_landmarks.dat'\n",
    "img_to_encoding.align = openface.AlignDlib(facePredictor)\n",
    "\n",
    "def detect_largest_face(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_boxes = detect_largest_face.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    face_areas = [w*h for x, y, w, h in face_boxes]\n",
    "    return face_boxes[face_areas.index(max(face_areas))]\n",
    "\n",
    "detect_largest_face.face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# def get_cropped_face(img_path):\n",
    "#     img = cv2.imread(img_path, 1)\n",
    "#     #TODO: take out manual resizing completely\n",
    "#     #\n",
    "#     face_box = detect_largest_face(img)\n",
    "#     img_cropped = img[y:y+h, x:x+w]\n",
    "#     img_cropped = cv2.resize(img_cropped, (96, 96))\n",
    "#     cv2.imshow('cropped', img_cropped)\n",
    "#     cv2.waitKey()\n",
    "#     return img_cropped, face_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detection took 0.156666994095 secs\n",
      "Face alignment part 1 took 3.81469726562e-06 secs\n",
      "Face alignment part 2 took 0.00606799125671 secs\n",
      "Face alignment took 0.00649189949036 secs\n",
      "Forward pass took 0.0362501144409 secs\n",
      "Face detection took 0.0595920085907 secs\n",
      "Face alignment part 1 took 3.09944152832e-06 secs\n",
      "Face alignment part 2 took 0.00386786460876 secs\n",
      "Face alignment took 0.00402212142944 secs\n",
      "Forward pass took 0.0323541164398 secs\n",
      "Face detection took 0.0419321060181 secs\n",
      "Face alignment part 1 took 1.90734863281e-06 secs\n",
      "Face alignment part 2 took 0.00418496131897 secs\n",
      "Face alignment took 0.00432586669922 secs\n",
      "Forward pass took 0.0394940376282 secs\n",
      "Face detection took 0.0184190273285 secs\n",
      "Face alignment part 1 took 2.86102294922e-06 secs\n",
      "Face alignment part 2 took 0.00508904457092 secs\n",
      "Face alignment took 0.00527381896973 secs\n",
      "Forward pass took 0.0442559719086 secs\n"
     ]
    }
   ],
   "source": [
    "database = {}\n",
    "database['ketan'] = img_to_encoding('/Users/ketanagrawal/Desktop/image-test/ketan-1.jpg', model)\n",
    "database['sid'] = img_to_encoding('/Users/ketanagrawal/Desktop/image-test/sid-1.jpeg', model)\n",
    "database['parker'] = img_to_encoding('/Users/ketanagrawal/Desktop/image-test/parkerface.jpeg', model)\n",
    "database['aditya'] = img_to_encoding('/Users/ketanagrawal/Desktop/image-test/aditya-1.jpg', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11695807  0.0479257  -0.02169226  0.08518837  0.03476244  0.1282892\n",
      "  -0.0044486  -0.226699    0.14711022 -0.04599588 -0.02680009  0.07860252\n",
      "  -0.0890829   0.04648315 -0.07148203  0.00265849 -0.070216    0.07054456\n",
      "  -0.00763653 -0.10913508  0.008445   -0.03583751 -0.05542693  0.14082219\n",
      "   0.09315073 -0.03337851 -0.10274325 -0.01445491 -0.00047704  0.05960917\n",
      "   0.17783956  0.06275218 -0.08754221 -0.07426174  0.02352531  0.11968327\n",
      "  -0.12123275 -0.10547408 -0.0139121   0.07404707  0.07273468  0.0941164\n",
      "   0.00544526 -0.03143573  0.0590634  -0.13778393  0.02667193 -0.00500333\n",
      "   0.08644373 -0.0814798  -0.02033384 -0.04388687  0.10945784 -0.10921692\n",
      "   0.03982809  0.13877183 -0.06106444  0.24127991 -0.01309673 -0.00177161\n",
      "  -0.11365721  0.14090435  0.14560008 -0.13199224  0.06219028  0.10357567\n",
      "   0.04821612 -0.08780783 -0.15268555  0.15043259  0.05257808  0.15013172\n",
      "   0.02377413  0.07691078  0.00340313  0.02818717 -0.03454704 -0.09512714\n",
      "  -0.10596297 -0.10750516  0.01972636  0.17497547  0.02106167 -0.00776615\n",
      "  -0.09239972 -0.02610186  0.00644196 -0.04095658 -0.08838519 -0.05504252\n",
      "  -0.00032389 -0.01301208  0.11309287 -0.05482616  0.14843126  0.05743548\n",
      "  -0.06896828 -0.00655557  0.04318861  0.12293422 -0.00325702 -0.08601569\n",
      "  -0.04710708 -0.02930225 -0.06242167  0.11076207 -0.00185279 -0.17291354\n",
      "  -0.09076112  0.08514162  0.02970646  0.02206175  0.07996388 -0.04562465\n",
      "   0.00102812  0.03512584  0.00333163  0.09045057  0.05510951  0.04380744\n",
      "   0.09134878 -0.06562272 -0.02492725 -0.12497678 -0.1699839   0.21329382\n",
      "  -0.09949098  0.02986559]]\n"
     ]
    }
   ],
   "source": [
    "print database['ketan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADED FUNCTION: who_is_it# GRADED \n",
    "\n",
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the happy house by finding who is the person on the image_path image.\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    database -- database containing image encodings along with the name of the person on the image\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "    identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "    ## Step 1: Compute the target \"encoding\" for the image. Use img_to_encoding() see example above. ## (≈ 1 line)\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    start = time.time()\n",
    "    ## Step 2: Find the closest encoding ##\n",
    "    \n",
    "    # Initialize \"min_dist\" to a large value, say 100 (≈1 line)\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Loop over the database dictionary's names and encodings.\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Compute L2 distance between the target \"encoding\" and the current \"emb\" from the database. (≈ 1 line)\n",
    "        dist = np.linalg.norm(db_enc - encoding, ord=2)\n",
    "        print \"distance from photo to %s is %s\" % (name, dist)\n",
    "        # If this distance is less than the min_dist, then set min_dist to dist, and identity to name. (≈ 3 lines)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Not in the database.\")\n",
    "    else:\n",
    "        print (\"it's \" + str(identity) + \", the distance is \" + str(min_dist))\n",
    "    print \"Face identification took %s secs\" % (time.time() - start)    \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face detection took 0.158617973328 secs\n",
      "Face alignment part 1 took 5.96046447754e-06 secs\n",
      "Face alignment part 2 took 0.00606918334961 secs\n",
      "Face alignment took 0.00665092468262 secs\n",
      "Forward pass took 0.036406993866 secs\n",
      "distance from photo to ketan is 1.4537299\n",
      "distance from photo to parker is 0.42841315\n",
      "distance from photo to aditya is 1.2925551\n",
      "distance from photo to sid is 1.5167753\n",
      "it's parker, the distance is 0.42841315\n",
      "Face identification took 0.00159502029419 secs\n",
      "Total time taken: 0.20481300354\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "who_is_it('/Users/ketanagrawal/Desktop/image-test/parker-2.jpg', database, model)\n",
    "# who_is_it('/Users/ketanagrawal/Desktop/image-test/ketan-5.jpg', database, model)\n",
    "end = time.time()\n",
    "print \"Total time taken: %s\" % (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
